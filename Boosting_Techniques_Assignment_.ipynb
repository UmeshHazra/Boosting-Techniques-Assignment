{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-015  \n",
        "Boosting Techniques | Assignment\n",
        "\n",
        "Name:- Umesh Hazra\n",
        "\n",
        "Email:- hazraumesh27@gmail.com\n",
        "\n",
        "Assignment Name:- Boosting Techniques Assignment"
      ],
      "metadata": {
        "id": "r4gEaAyy8cEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it\n",
        "improves weak learners.\n",
        "\n",
        "Answer:-  \n",
        "\n",
        "## What is Boosting in Machine Learning?\n",
        "\n",
        "**Boosting** is an ensemble technique in machine learning that combines multiple weak learners to build a strong predictive model. A weak learner is a model that performs slightly better than random chance (for example, a small decision tree). Boosting trains these weak learners sequentially, each one learning from the mistakes of its predecessor.\n",
        "\n",
        "### How Boosting Works\n",
        "\n",
        "- **Sequential Learning**: In boosting, learners are trained one after another, and each new learner focuses on the data points that previous learners misclassified.\n",
        "- **Weighted Data**: Each time a weak learner is trained, the data points that were mispredicted by the previous learners are given higher weights. This means subsequent models pay more attention to difficult cases.\n",
        "- **Combining Predictions**: The final prediction is made by aggregating the outputs (often weighted) of all weak learners, resulting in a strong ensemble.\n",
        "\n",
        "### How Boosting Improves Weak Learners\n",
        "\n",
        "Boosting transforms **weak learners**—which are only slightly better than random guessing—into a strong overall model by:\n",
        "\n",
        "- **Focusing on Hard Cases**: By concentrating on misclassified samples, boosting ensures that the ensemble learns patterns that single weak learners might miss.\n",
        "- **Reducing Bias**: Combining several weak learners decreases their individual biases, allowing the overall model to better fit the training data.\n",
        "- **Weighted Voting/Aggregation**: Boosted models often use a weighted voting system where more accurate learners contribute more to the final prediction.\n",
        "\n",
        "#### Example of a Boosting Algorithm\n",
        "\n",
        "The most common boosting algorithm is **AdaBoost (Adaptive Boosting)**. In AdaBoost:\n",
        "\n",
        "1. The first learner is trained on the entire dataset.\n",
        "2. Misclassified points are given higher weights.\n",
        "3. The next learner is trained with this weighted data, so it focuses more on those misclassified samples.\n",
        "4. This process repeats for a set number of weak learners.\n",
        "5. Final prediction is the weighted sum of all learners' predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "1Qbyh6sA8gZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient\n",
        "Boosting in terms of how models are trained?\n",
        "\n",
        "Answer:-\n",
        "\n",
        "## AdaBoost vs Gradient Boosting: Model Training Differences\n",
        "\n",
        "### AdaBoost (Adaptive Boosting)\n",
        "\n",
        "- **Training Approach**: AdaBoost starts by training a weak learner (often a shallow decision tree) on the full data, giving equal weight to all samples. After each round, it *increases the weights* for misclassified samples, so future learners pay more attention to data points that previous models got wrong.\n",
        "- **Sequential Weighting**: Each new learner is trained on data with updated sample weights reflecting prior errors. This continues for each weak learner in the sequence.\n",
        "- **Final Model**: The ensemble combines all weak learners using a weighted vote, where each model's contribution is scaled according to its accuracy.[1][2][3]\n",
        "\n",
        "### Gradient Boosting\n",
        "\n",
        "- **Training Approach**: Gradient Boosting also builds learners sequentially, but it does so by fitting each new weak learner to the *residuals* (errors) left by previous models. Rather than manipulating sample weights, Gradient Boosting focuses on minimizing a differentiable loss function using gradient descent techniques.\n",
        "- **Residual Correction**: After each model, the algorithm computes the gradient of the loss function (the amount by which predictions should change to reduce error) and trains the next learner to predict these residuals or negative gradients.\n",
        "- **Final Model**: Each learner’s predictions are scaled (often by a learning rate) and then *added* to the previous predictions to update the ensemble. This continues until a stopping criterion is met.[4][5][6]\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Aspect                | AdaBoost                                           | Gradient Boosting                        |\n",
        "|-----------------------|---------------------------------------------------|-------------------------------------------|\n",
        "| **Focus**             | Re-weights misclassified samples                   | Fits new models to minimize loss function |\n",
        "| **Learner Training**  | Trains on reweighted data                          | Trains on residuals/gradients             |\n",
        "| **Error Handling**    | Prioritizes hard-to-classify points                | Learns from errors (gradients)            |\n",
        "| **Loss Function**     | Typically exponential (can be generalized)         | Arbitrary, differentiable loss functions  |\n",
        "| **Update Process**    | Weighted voting of learner outputs                 | Summing predictions scaled by learning rate|\n",
        "| **Typical Weak Learner** | Shallow decision trees                        | Shallow decision trees or other models    |\n",
        "\n"
      ],
      "metadata": {
        "id": "458I3vAr8wky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Answer:-\n",
        "\n",
        "## How Regularization Helps in XGBoost\n",
        "\n",
        "**Regularization in XGBoost** is a core feature designed to prevent overfitting and improve model generalization. XGBoost incorporates several regularization techniques into its objective function and model-building process:\n",
        "\n",
        "### Main Regularization Techniques in XGBoost\n",
        "\n",
        "- **L1 (Lasso) Regularization (`alpha` or `reg_alpha`)**\n",
        "  - Adds the absolute values of the feature weights to the loss function.\n",
        "  - Encourages sparsity by driving some feature weights to zero, resulting in a simpler and more interpretable model.\n",
        "\n",
        "- **L2 (Ridge) Regularization (`lambda` or `reg_lambda`)**\n",
        "  - Adds the squared values of the feature weights to the loss function.\n",
        "  - Encourages small, evenly distributed weights rather than driving them to zero, thus reducing model complexity.\n",
        "\n",
        "- **Early Stopping (`early_stopping_rounds`)**\n",
        "  - Stops training if a validation metric does not improve for a given number of rounds.\n",
        "  - Prevents the model from continuing to fit noise and overfitting the data.\n",
        "\n",
        "- **Tree-Specific Parameters**\n",
        "  - **`min_child_weight`**: Requires each leaf node to have a minimum sum of instance weights, reducing tree complexity and overfitting risk.\n",
        "  - **`gamma`**: Specifies the minimum loss reduction required to make a further partition, leading to more conservative splits and simpler models."
      ],
      "metadata": {
        "id": "VcujRqJf87Ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is CatBoost considered efficient for handling\n",
        "categorical data?\n",
        "\n",
        "Answer:-\n",
        "\n",
        "## CatBoost’s Efficiency with Categorical Data\n",
        "\n",
        "CatBoost is considered highly efficient for handling categorical data due to several unique features and design principles:\n",
        "\n",
        "### 1. **Native Categorical Feature Support**\n",
        "- CatBoost can process categorical variables directly, **without any explicit preprocessing like label encoding or one-hot encoding** required by most other algorithms.[1][4]\n",
        "- It automatically converts categorical values into numerical representations using statistical methods and combinations of features, making the process seamless and less error-prone.[5]\n",
        "\n",
        "### 2. **Advanced Encoding Techniques**\n",
        "- CatBoost uses **ordered boosting** and the concept of **combinatorial categorical features**, which means it not only encodes single categorical columns but also leverages interactions between multiple categorical and numerical features to generate meaningful representations.[1][5]\n",
        "- Categorical values are transformed based on various statistics derived from the target variable and the combinations of feature values, which improves the quality of encoding and predictive power.\n",
        "\n",
        "### 3. **Prevents Target Leakage**\n",
        "- The encoding process is designed to avoid target leakage by using a special permutation-driven approach, meaning statistics used for encoding are calculated per permutation and are not directly derived from the full dataset, which preserves the validity of training.[5]\n",
        "\n",
        "### 4. **Efficient and Robust Training**\n",
        "- CatBoost’s handling reduces the likelihood of overfitting that often occurs with one-hot encoding, especially for high-cardinality features (features with many unique categories).[4]\n",
        "- It is particularly well-suited for large-scale datasets containing both numeric and categorical features, and its automation simplifies workflows for practitioners.[4]\n",
        "\n"
      ],
      "metadata": {
        "id": "RGDP3NEG9H8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting\n",
        "techniques are preferred over bagging methods?\n",
        "\n",
        "Answer:-\n",
        "\n",
        "## Real-World Applications Where Boosting Is Preferred Over Bagging\n",
        "\n",
        "Boosting methods (such as AdaBoost, Gradient Boosting, XGBoost, and CatBoost) are especially preferred in scenarios where *high prediction accuracy* and *sequential error correction* are crucial. Here are several key real-world applications:\n",
        "\n",
        "### 1. **Medical Diagnosis**\n",
        "- Boosting techniques are often chosen for medical prediction tasks (e.g., disease classification, risk prediction) because they can uncover subtle patterns by sequentially correcting misclassifications, thus achieving higher accuracy when data quality is high and minimizing bias is critical.[1][5]\n",
        "\n",
        "### 2. **Credit Scoring and Fraud Detection**\n",
        "- In finance, boosting models perform well for credit risk analysis and fraud detection, where misclassifications have financial repercussions and accuracy outweighs the dangers of overfitting. Boosting’s ability to focus on hard-to-classify cases is beneficial for detecting rare fraudulent transactions.[5][1]\n",
        "\n",
        "### 3. **Customer Churn Prediction**\n",
        "- Telecom and subscription services use boosting for predicting customer churn, as these models adapt to complex decision boundaries and deliver superior predictive performance important for business strategy.[3][5]\n",
        "\n",
        "### 4. **Web Search Rankings and Recommendation Systems**\n",
        "- Boosting (particularly Gradient Boosted Decision Trees and XGBoost) powers many search engine ranking and recommendation algorithms, where incremental gains in accuracy from sequential error correction drive user engagement and business results.[1]\n",
        "\n",
        "### 5. **Image Recognition and Natural Language Processing**\n",
        "- When high accuracy in classification is needed (e.g., facial recognition, sentiment analysis, spam detection), boosting is preferred because it can build complex decision functions that bagging sometimes cannot replicate, and it thrives on low-bias base learners.[6]\n",
        "\n"
      ],
      "metadata": {
        "id": "S7YAmFyw9Zfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:  \n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset  \n",
        "● Print the model accuracy\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "BRhYMaRY9pjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.2, random_state=42)\n",
        "\n",
        "classifier = AdaBoostClassifier(random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53O3zJ0g9uR2",
        "outputId": "f2632874-affb-432e-a60b-91edf2634bd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer Dataset: 96.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:  \n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "Answer:-  "
      ],
      "metadata": {
        "id": "zxoiKyDm-asz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = GradientBoostingRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R-squared score on California Housing Dataset: {:.4f}\".format(r2))"
      ],
      "metadata": {
        "id": "PhWYj094-fZ1",
        "outputId": "0f72d02d-6a50-4e06-947c-a1ee587afb92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score on California Housing Dataset: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:  \n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset  \n",
        "● Tune the learning rate using GridSearchCV  \n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "o-DALxK4-lXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.2, random_state=42)\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
        "random_state=42)\n",
        "\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
        "\n",
        "grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='accuracy',\n",
        "n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"XGBoost Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "id": "nkVtkC0x-pnQ",
        "outputId": "54222b27-46f2-41f5-90d6-5b7bcbe0d3c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "XGBoost Classifier Accuracy on Breast Cancer Dataset: 95.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:  \n",
        "● Train a CatBoost Classifier  \n",
        "● Plot the confusion matrix using seaborn  \n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "LP7wXCeH-wGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.2, random_state=42)\n",
        "\n",
        "clf = CatBoostClassifier(verbose=0, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"CatBoost Classifier Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U9HKJoRj-zzP",
        "outputId": "1d96ab70-c7d9-4de2-930a-8518817331ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict\n",
        "loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both\n",
        "numeric and categorical features. Describe your step-by-step data\n",
        "science pipeline using boosting techniques:  \n",
        "● Data preprocessing & handling missing/categorical values  \n",
        "● Choice between AdaBoost, XGBoost, or CatBoost  \n",
        "● Hyperparameter tuning strategy  \n",
        "● Evaluation metrics you'd choose and why  \n",
        "● How the business would benefit from your model\n",
        "\n",
        "Answer:-\n",
        "\n",
        "## Step-by-Step Data Science Pipeline for Loan Default Prediction (Imbalanced, Mixed Data)\n",
        "\n",
        "***\n",
        "\n",
        "### 1. **Data Preprocessing**\n",
        "\n",
        "- **Missing Values**\n",
        "  - *Numeric features*: Impute missing values using median (robust to outliers) or advanced techniques like KNN imputation if patterns exist.\n",
        "  - *Categorical features*: Impute using the most frequent category or introduce a new category (e.g., “Unknown”) to retain missingness information.\n",
        "\n",
        "- **Categorical Variables**\n",
        "  - Use encoding strategies compatible with chosen boosting algorithms:\n",
        "    - For **AdaBoost/XGBoost**: Apply label encoding for ordinal data and target/one-hot encoding for nominal data with few unique values. For high-cardinality, use target or frequency encoding.\n",
        "    - For **CatBoost**: No manual encoding needed—CatBoost handles raw categorical features natively.\n",
        "\n",
        "- **Imbalanced Classes**\n",
        "  - Use oversampling (SMOTE), undersampling, or class weight adjustments. Most boosting algorithms allow class weights to be specified to mitigate bias toward the majority class.\n",
        "\n",
        "- **Feature Scaling**\n",
        "  - Scaling is less critical for tree-based boosting, but normalization can help with other models or for feature comparison.\n",
        "\n",
        "***\n",
        "\n",
        "### 2. **Choice of Boosting Algorithm**\n",
        "\n",
        "- **AdaBoost** is less optimal for complex, heterogeneous datasets—especially with categorical features and missing data.\n",
        "- **XGBoost**\n",
        "  - Handles numeric data efficiently.\n",
        "  - Can manage missing values internally.\n",
        "  - Requires manual encoding for categorical data.\n",
        "- **CatBoost** *(Recommended)*\n",
        "  - Designed for heterogeneous, mixed-type datasets.\n",
        "  - Natively manages categorical variables, missing values, and is robust to class imbalance with “auto” handling.\n",
        "  - Requires less manual preprocessing, reducing risk of information leakage or encoding errors.\n",
        "\n",
        "***\n",
        "\n",
        "### 3. **Hyperparameter Tuning Strategy**\n",
        "\n",
        "- Use **GridSearchCV** or **RandomizedSearchCV** for systematic hyperparameter exploration.\n",
        "  - Key hyperparameters to tune:\n",
        "    - Learning rate\n",
        "    - Number of estimators (trees)\n",
        "    - Maximum tree depth\n",
        "    - Regularization parameters (`reg_alpha`, `reg_lambda` for XGBoost; L2_leaf_reg for CatBoost)\n",
        "    - Boosting type (for CatBoost: Ordered, Plain)\n",
        "    - Categorical feature handling options\n",
        "  - Prefer stratified cross-validation to preserve class ratios due to imbalance.\n",
        "  - BayesSearchCV (from `scikit-optimize`) can be used for efficient, probabilistic searching.\n",
        "\n",
        "***\n",
        "\n",
        "### 4. **Evaluation Metrics**\n",
        "\n",
        "- **Primary Metrics** (suited for imbalanced classification):\n",
        "  - **AUC-ROC (Area Under Receiver Operating Characteristic Curve)**: Measures the ability to distinguish default vs. non-default across thresholds.\n",
        "  - **F1 Score**: Balances precision and recall, especially important for detecting minority class (defaults).\n",
        "  - **Recall/Sensitivity**: Critical to minimize false negatives so defaults are not missed.\n",
        "  - **Precision**: To avoid flagging too many non-defaulters as risky.\n",
        "- **Secondary Metrics**:\n",
        "  - Confusion Matrix\n",
        "  - PR (Precision-Recall) Curve\n",
        "  - Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "***\n",
        "\n",
        "### 5. **Business Value/Impact**\n",
        "\n",
        "- **Risk Mitigation:** By accurately predicting loan defaults, the company reduces financial losses and controls risk exposure.\n",
        "- **Credit Policy Optimization:** The model enables tailored interest rates and limits, improving customer targeting and retention.\n",
        "- **Fraud Detection Synergy:** Boosting techniques capture subtle patterns, making fraud and risky behaviors easier to detect.\n",
        "- **Regulatory Compliance:** Reliable predictions provide better documentation for compliance and audit trails.\n",
        "- **Improved Profitability:** Proactive risk management translates to lower provision costs and higher net income."
      ],
      "metadata": {
        "id": "LiJL2bAf_Jtj"
      }
    }
  ]
}